\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Multilayer Perceptron}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Methods}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Treatment of Data}{1}}
\@writefile{toc}{\contentsline {paragraph}{Splitting}{1}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}MLP setup}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schema of the MLP described in the project instructions\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mlp}{{1}{2}}
\@writefile{toc}{\contentsline {paragraph}{Effects of parameters on convergence speed and errors}{2}}
\newlabel{fig:small_eta}{{2a}{2}}
\newlabel{sub@fig:small_eta}{{a}{2}}
\newlabel{fig:larger_eta}{{2b}{2}}
\newlabel{sub@fig:larger_eta}{{b}{2}}
\newlabel{fig:decrease_eta}{{2c}{2}}
\newlabel{sub@fig:decrease_eta}{{c}{2}}
\newlabel{fig:increase_momentum}{{2d}{2}}
\newlabel{sub@fig:increase_momentum}{{d}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This figure exemplifies the effects of increasing the initial learning rate $\eta _0$ and then decreasing it every epoch, as well as the effect of introducing a momentum term stabilizing the gradient descent.\relax }}{2}}
\newlabel{fig:effects_learning_rate}{{2}{2}}
\newlabel{fig:h115}{{3a}{3}}
\newlabel{sub@fig:h115}{{a}{3}}
\newlabel{fig:h120}{{3b}{3}}
\newlabel{sub@fig:h120}{{b}{3}}
\newlabel{fig:h130}{{\caption@xref {fig:h130}{ on input line 81}}{3}}
\newlabel{fig:h140}{{\caption@xref {fig:h140}{ on input line 88}}{3}}
\newlabel{fig:h150}{{\caption@xref {fig:h150}{ on input line 95}}{3}}
\newlabel{fig:h160}{{3f}{3}}
\newlabel{sub@fig:h160}{{f}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plotting graphs of the logistical error for different numbers of hidden units shows different performances and convergence in different numbers of epochs. No general pattern emerged in the depicted plots generated with early stopping. However, the model complexity should increase with the number of hidden units leading to an overfitting of the training set. More on this in the next section.\relax }}{3}}
\newlabel{fig:effects_h1}{{3}{3}}
\newlabel{subfig:overfit1}{{4a}{4}}
\newlabel{sub@subfig:overfit1}{{a}{4}}
\newlabel{subfig:overfit2}{{4b}{4}}
\newlabel{sub@subfig:overfit2}{{b}{4}}
\newlabel{subfig:overfit3}{{4c}{4}}
\newlabel{sub@subfig:overfit3}{{c}{4}}
\newlabel{subfig:overfit4}{{4d}{4}}
\newlabel{sub@subfig:overfit4}{{d}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example examples of overfitting for different numbers of hidden units\relax }}{4}}
\newlabel{fig:overfitting}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Effects of parameter choice on overfitting}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Results}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Choice of parameters for subtasks and discussion of performance on test set}{4}}
\newlabel{subfig:result3}{{5a}{5}}
\newlabel{sub@subfig:result3}{{a}{5}}
\newlabel{subfig:result4}{{5b}{5}}
\newlabel{sub@subfig:result4}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sample graphs of a run of the final parametrization of the MLP on both subtasks annotated with the performance of test with mean and standard deviation\relax }}{5}}
\newlabel{fig:performance_result}{{5}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Misclassified patterns}{5}}
\newlabel{subfig:misclass_3}{{6a}{5}}
\newlabel{sub@subfig:misclass_3}{{a}{5}}
\newlabel{subfig:misclass_4}{{6b}{5}}
\newlabel{sub@subfig:misclass_4}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Examples of misclassified patterns for the 4-9 problem\relax }}{5}}
\newlabel{fig:misclassification}{{6}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Support Vector Machine}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Methods}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Treatment of Data}{5}}
\@writefile{toc}{\contentsline {paragraph}{Splitting}{5}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}SVM setup}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Results}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Summed up misclassification scores for range of parameters $C$ and $\tau $ depicted after 10-fold crossvalidation. The smallest total score 59 was recorded for $\tau =.128$ and $C=2$. The range of values is displayed up to a maximum score of 300 which represents 5\% misclassifications on the validation data over the 10 folds.\relax }}{6}}
\newlabel{fig:scores}{{7}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}SVM criterion and Convergence criterion}{6}}
\@writefile{toc}{\contentsline {paragraph}{SVM Performance}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Performance comparison of SVM and MLP}{6}}
\newlabel{subfig:svm_criterion}{{8a}{7}}
\newlabel{sub@subfig:svm_criterion}{{a}{7}}
\newlabel{subfig:logarithmic}{{8b}{7}}
\newlabel{sub@subfig:logarithmic}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces SMO run on full training set with parameters found through cross-validation\relax }}{7}}
\newlabel{fig:criterions}{{8}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Bar plot of 0/1 error for training and testing comparing final parametrizations MLP and SVM (including the standard deviation for the MLP)\relax }}{7}}
\newlabel{fig:comparison}{{9}{7}}
