\section{Support Vector Machine}

\subsection{Methods}

\subsubsection{Treatment of Data}
\paragraph{Splitting}
For the implementation of the SMO algorithm the training data had not to be split into fixed training and validation sets. Instead the imperative was to implement 10-fold crossvalidation for parameter selection of tao and C. In order to achieve this task and save time we used the KFold() method of the python scikit library. This method thus splits the dataset into 10 consecutive folds without shuffling We split the data into 10 bins and talk one of them alternatingly as a validation set.
Split dataset into k consecutive folds (without shuffling).

Each fold is then used a validation set once while the k - 1 remaining fold form the training set.

\paragraph{Preprocessing}
Regarding preprocessing we used the same normalization as with the Multilayer Perceptron.

\subsubsection{SVM setup}
include all implementation details including choice of C and of $tao$\\
evaluate 10-fold CV scores for all combinations of the matrix

Plot the SVM criterion as function of SMO iterations (only
every 20 SMO steps). Additionally, plot the value of the convergence criterion,
based on KKT condition violations (see additional note). For the latter plot,
the vertical axis should have a logarithmic scale.
\subsection{Results}